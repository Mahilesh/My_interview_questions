Kubernetes
==========

1. Kubernetes architecture
- Kubernetes follows a master-worker architecture (also known as control plane and nodes). It consists of the following key components:
- Controller plane (Master nodes), Data plane (Worker nodes)
  - In controller plane, consistes of components like API-server, Schedular, etcd, Controller Manager, Cloud Controller Manager
      - API-server - Acts as the entry point for all Kubernetes commands (kubectl, REST API). It handles authentication, authorization, and communicates with all components.
      - Schedular - Assigns newly created pods to available worker nodes based on resource availability, affinity/anti-affinity rules, taints/tolerations, etc.
      - Etcd - It is like database, A distributed key-value store used to store all cluster data, configurations, and state. It acts as the single source of truth.
      - Controller Manager -  Runs various controllers that handle routine tasks like:
          - Node controller (check node status)
          - Replication controller (ensure desired pod count)
          - Endpoint controller, etc. 
      - CLoud Controlller Manager - Kubernetes interact with the cloud provider's APIs. It separates out the cloud-specific logic from the rest of the Kubernetes core.
  - In Data plane, consistes of components like Kubelet, Kube proxy, Container runtime interface (CRI)
      - Kubelet - An agent on each node that ensures containers are running as per pod specs.
      - Kube proxy - Manages networking and load-balancing across pods and services on a node.
      - Container runtime interface - Pulls and runs containers inside pods.

2. Deployment vs stateful set
- Deployments are ideal for stateless applications where pod identity doesn't matter. 
  Ex:- web servers, APIs
- StatefulSets are used for stateful apps where ordered startup and stable identities are crucial, especially with persistent storage. 
  Ex:- databases, Kafka, Zookeeper

3. Stateful Vs Stateless
- A Stateless app (Deployment) doesn’t store client context between requests — it's easier to scale and recover. 
  Ex:- NGINX or Apache Web Server
- A Stateful app (Statefulset) requires maintaining session or data consistency, needing persistent storage and careful orchestration.
  Ex:- MongoDB / MySQL

4. How do you manage data of stateless application?
- In stateless apps, we manage data by storing it outside the application. This includes using external databases, caching layers for sessions, object storage for files, and messaging queues for decoupled processing. The app remains stateless by not relying on internal memory or disk.

5. If there is file which is being used by 2 customers, and need to deploy that file in k8s cluster and on prem as well, how to do that?
- Method1:- Use a central storage (e.g., S3, NFS) to keep the shared file and sync it to both Kubernetes (via volume or initContainer) and on-prem (via script or cron job).
- Method2:- Include the file in your CI/CD pipeline to deploy it to both environments consistently.
  - Commands to use
      Kubernetes - kubectl cp
      On-premise - scp or rsync 

6. How to deploy an app to k8s cluster in terms of app deploy only ( basically explain CD part)
- Step1:- Prepare Kubernetes YAML manifests - deployment.yaml, service.yaml, configmap.yaml, etc.
- Step2:- Push updated image to container registry
- Step3:- Update the YAML with the new image tag
- Step4:- Apply the YAML to the cluster using #kubectl

7. If secret is stored in vault inside a pod and that pod is down then how to tsg?
- If a pod that pulls secrets from Vault is down, I first check the pod status and logs, then validate Vault connectivity, the Kubernetes auth configuration, and token validity. I also inspect volume mounts or Vault agent init containers. Based on findings, I restart the pod or reinitiate auth if required.

Step1:- Check pod status            
  #kubectl get pod <pod-name> -n <namespace>
  #kubectl describe pod <pod-name> -n <namespace>
Step2:- Check Logs (Application + Vault Agent)
  #kubectl logs <pod-name> -n <namespace>
  #kubectl logs <pod-name> -c vault-agent -n <namespace> 
Step3:- Check vault connectivity
  #curl -s $VAULT_ADDR/v1/sys/health
Step4:- Validate Vault Token / Auth Role
  #vault read auth/kubernetes/role/<your-role>
Step5:- Check InitContainer or VolumeMounts (if secrets are injected)
  #kubectl get pod <pod-name> -o yaml
Step6;- Redeploy Pod or Rollout Restart
  #kubectl rollout restart deployment <your-deployment> -n <namespace>

8. Contents written inside deployment.yaml or heml chart
- deployment.yaml, we define how our application should be deployed in the Kubernetes cluster. It includes metadata like the app name and labels, the number of replicas, the container image, ports, environment variables, and volume mounts. It may also include health checks, resource limits, and affinity rules.
- Helm - deployment.yaml - becomes a template using placeholders (like {{ .Values.image.repository }}), and values are dynamically pulled from "values.yaml" to support reusability and parameterization across environments.

9. In Kubernetes, if a pod is in a pending state, how do you troubleshoot?
- Step1:- #kubectl describe pod <name-of-the-pod> -n <namespace>
- Step2:- #kubectl get nodes, #kubectl describe node <node-name>
- Step3:- #kubectl get pvc -n <namespace>         ----> (Check PVCs (if using volumes))
- Step4:- #kubectl describe node <node-name> | grep -i taint          ----> Pod may not tolerate node taints.
- Step5:- If you have master node access, then login to that node and check schedular logs
#kubectl get pods -n kube-system | grep scheduler          (to find the schedular)
#kubectl logs -n kube-system kube-scheduler-<pod> 

10. An application upgrade caused downtime even though you had rolling updates configured. What advanced strategies would you apply to ensure zero-downtime deployments next time?

- Use readiness probes to prevent traffic to unhealthy pods.
- Implement blue-green or canary deployments using tools like Argo Rollouts or Flagger.
- Introduce preStop hooks for graceful shutdown.
- Ensure proper resource limits and startup/liveness probes are configured.
- Validate all configurations in staging with production-like traffic using load testing.

11. Your service mesh sidecar (e.g., Istio Envoy) is consuming more resources than the app itself. How do you analyze and optimize this setup?

- Use metrics from Prometheus/Grafana or Istio Dashboard to monitor sidecar resource usage.
- Analyze access logs and tracing (Jaeger/Zipkin) to find excessive routing or retries.
- Tune Envoy config (e.g., connection pool, logging level).
- Limit sidecar injection to only required namespaces/pods.
- Consider switching to a lighter mesh if overkill for the use case.

12. You need to create a Kubernetes operator to automate complex application lifecycle events. How do you design the CRD and controller loop logic?

- Define a Custom Resource Definition (CRD) that describes your app's lifecycle (e.g., phase, version).
- Write a controller (in Go using Kubebuilder or Operator SDK) to watch CR changes.
- Implement reconciliation loop logic to check desired vs actual state and take actions.
- Handle error retries and status updates for observability.
- Use finalizers for clean teardown logic.

13. Multiple nodes are showing high disk IO usage due to container logs. What Kubernetes features or practices can you apply to avoid this scenario?

- Use log rotation with `logrotate` or configure container runtime logging limits.
- Offload logs using Fluentd/Fluent Bit to a centralized log system like ELK or Loki.
- Use ephemeral storage limits on pods.
- Switch to sidecar log shipping model if applicable.
- Enable journald or structured logging to reduce disk usage.

14. Your Kubernetes cluster's etcd performance is degrading. What are the root causes and how do you ensure etcd high availability and tuning?

- Root causes: large number of objects, frequent writes, disk latency, slow backups.
- Use SSD-backed persistent volumes for etcd.
- Keep etcd cluster size to odd number (3 or 5).
- Tune etcd compaction and defragmentation intervals.
- Monitor etcd metrics and use alerts for latency, fsync, and DB size.

15. You want to enforce that all images used in the cluster must come from a trusted internal registry. How do you implement this at the policy level?

- Use an Admission Controller like OPA/Gatekeeper to validate image registry.
- Create policies that block images not matching `myregistry.company.com/*`.
- Use Kubernetes PodSecurityPolicy (deprecated), PodSecurity Admission or Kyverno to enforce registry constraints.
- Integrate CI pipeline to tag and push only approved images.

16. You're managing multi-region deployments using a single Kubernetes control plane. What architectural considerations must you address to avoid cross-region latency and single points of failure?

- Use regional clusters with federation or multi-cluster tools like KubeFed or GKE Hub.
- Deploy control plane closer to the app/data location to avoid latency.
- Use global load balancer or DNS-based routing (e.g., ExternalDNS).
- Ensure high availability by replicating control plane and etcd.
- Avoid single point of failure by distributing workloads and control plane components.

17. During peak traffic, your ingress controller fails to route requests efficiently. How would you diagnose and scale ingress resources effectively under heavy load?

- Check ingress pod logs and metrics (CPU, memory, connections).
- Use horizontal pod autoscaler for ingress controller.
- Tune NGINX/HAProxy buffer sizes and connection timeouts.
- Implement rate limiting and caching.
- Use dedicated node pool for ingress traffic.

18. Asked about k8s (deployment, services, and configs)

- Deployment: manages replica sets, rolling updates, image versioning.
- Services: expose pods to other services or external traffic.
- Configs: use ConfigMaps and Secrets to externalize configuration.

19. Pod disruption budget

- Defines how many pods can be voluntarily disrupted at a time.
- Helps maintain availability during voluntary events like upgrades.
- Example: maxUnavailable: 1 or minAvailable: 2.

20. About K8s Architecture and tell me the workflow?

- Kube-apiserver receives user requests.
- Scheduler schedules pods onto nodes.
- Controller manager manages replicas, endpoints.
- etcd stores cluster state.
- Kubelet runs on each node and reports to control plane.
- Kube-proxy handles networking and routing.

21. Tell about RBAC

- RBAC controls access to Kubernetes resources based on roles.
- Role/ClusterRole: define permissions.
- RoleBinding/ClusterRoleBinding: bind users/groups to roles.
- Ensures least-privilege access control.

22. In your projects, how many containers did you run? Can you give me a use case where you would run 4–5 containers in a pod?

- Typically 1-2 containers per pod, but used 4-5 in cases like:
  - Logging sidecar (Fluent Bit)
  - Monitoring sidecar (Prometheus exporter)
  - Main application
  - Proxy/mesh sidecar (Envoy)
  - Config watcher/reloader

23. How many containers can run in a pod?

- There is no strict limit; depends on node capacity and container requirements.
- Practically, 5-10 is manageable per pod for performance and debugging.

24. What will be your approach if pod.yaml failed?

- Check syntax: `kubectl apply -f pod.yaml --dry-run=client`
- Describe pod: `kubectl describe pod <name>`
- Logs: `kubectl logs <name>`
- Events: `kubectl get events`
- Fix issues like missing image, wrong config, port conflict.

25. Commands used in Kubernetes

- kubectl get pods/services/deployments
- kubectl describe pod <pod-name>
- kubectl logs <pod-name>
- kubectl apply -f <file.yaml>
- kubectl exec -it <pod> -- /bin/bash
- kubectl port-forward <pod> <port>:<port>

26. If the application which you are trying to deploy with Kubernetes crashes and you are not able to enter into the pod, what will be your approach?

- Check pod status: `kubectl get pods`
- Describe the pod: `kubectl describe pod <name>`
- View logs: `kubectl logs <name> --previous` (if it crashed)
- Check node resources and events.
- Verify image, config, secrets, and volume mounts.


